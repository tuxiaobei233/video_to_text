###  文本特征提取

#### 音频文本识别

使用一种高效的非自回归端到端语音识别框架Paraformer。具体来说使用VSD语音端点检测，检测输入音频中有效语音的起止时间点信息，减少无效语音带来的识别错误，分割音频缩短单次识别长度。使用ASR自动语音识别，识别出分割后的音频中的文本。使用标点检测对语音识别结果处理协助语音识别模块输出具有可读性的文本结果。同时确定每句话起止的时间戳用于后续对齐操作。

##### 语音端点检测

FSMN-Monophone VAD是语音端点检测模型，用于检测输入音频中有效语音的起止时间点信息，并将检测出来的有效音频片段输入识别引擎进行识别，减少无效语音带来的识别错误。

![2-1-3](.\assets\2-1-3.png)
图2-1-3：FSMN-Monophone VAD模型结构

FSMN-Monophone VAD模型结构如上图所示：模型结构层面，FSMN模型结构建模时可考虑上下文信息，训练和推理速度快，且时延可控；同时根据VAD模型size以及低时延的要求，对FSMN的网络结构、右看帧数进行了适配。在建模单元层面，speech信息比较丰富，仅用单类来表征学习能力有限，将单一speech类升级为Monophone。建模单元细分，可以避免参数平均，抽象学习能力增强，区分性更好。

##### 自动语音识别

我们使用Paraformer高效的非自回归端到端语音识别框架。Paraformer模型结构如下图所示，由 Encoder、Predictor、Sampler、Decoder 与 Loss function 五部分组成。Encoder可以采用不同的网络结构，例如self-attention，conformer，SAN-M等。Predictor 为两层FFN，预测目标文字个数以及抽取目标文字对应的声学向量。Sampler 为无可学习参数模块，依据输入的声学向量和目标向量，生产含有语义的特征向量。Decoder 结构与自回归模型类似，为双向建模（自回归为单向建模）。Loss function 部分，除了交叉熵（CE）与 MWER 区分性优化目标，还包括了 Predictor 优化目标 MAE。

![2-1-4](.\assets\2-1-4.png)
图2-1-4：Paraformer模型结构

其核心点主要有：Predictor 模块：基于 Continuous integrate-and-fire (CIF) 的 预测器 (Predictor) 来抽取目标文字对应的声学特征向量，可以更加准确的预测语音中目标文字个数。Sampler：通过采样，将声学特征向量与目标文字向量变换成含有语义信息的特征向量，配合双向的 Decoder 来增强模型对于上下文的建模能力。基于负样本采样的 MWER 训练准则。

##### 标点检测

我们使用 Controllable Time-delay Transformer 后处理框架中的标点模块。将识别到的语音转文字结果进行标点预测，使得文本更可读。

![2-1-5](.\assets\2-1-5.png)
图2-1-5：Controllable Time-delay Transformer 模型结构

Controllable Time-delay Transformer 模型结构如上图所示，由 Embedding、Encoder 和 Predictor 三部分组成。Embedding 是词向量叠加位置向量。Encoder可以采用不同的网络结构，例如self-attention，conformer，SAN-M等。Predictor 预测每个token后的标点类型。

在模型的选择上采用了性能优越的Transformer模型。Transformer模型在获得良好性能的同时，由于模型自身序列化输入等特性，会给系统带来较大时延。常规的Transformer可以看到未来的全部信息，导致标点会依赖很远的未来信息。这会给用户带来一种标点一直在变化刷新，长时间结果不固定的不良感受。基于这一问题，我们创新性的提出了可控时延的Transformer模型（Controllable Time-Delay Transformer, CT-Transformer），在模型性能无损失的情况下，有效控制标点的延时。

#### 图像文本识别

​	首先对每个视频帧进行**感知哈希phash**计算，得到图片相似度，过滤相似的帧。对过滤之后的帧进行文字检测，对于每一个文字框图像再进行**感知哈希phash**计算，过滤相似的文字框。最后进行文字识别得到图像文本。

​	**感知哈希phash**实现步骤：第一步，缩小尺寸。最快速的去除高频和细节，只保留结构明暗的方法就是缩小尺寸。将图片缩小到8x8的尺寸，总共64个像素。摒弃不同尺寸、比例带来的图片差异。第二步，简化色彩。将缩小后的图片，转为64级灰度。也就是说，所有像素点总共只有64种颜色。第三步，计算DCT（离散余弦变换）。DCT是把图片分解频率聚集和梯状形，虽然JPEG使用88的DCT变换，在这里使用3232的DCT变换。第四步，缩小DCT。虽然DCT的结果是3232大小的矩阵，但我们只要保留左上角的88的矩阵，这部分呈现了图片中的最低频率。第五步，计算平均值。计算所有64个值的平均值。第六步，进一步减小DCT计算哈希值。这是最主要的一步，根据8*8的DCT矩阵，设置0或1的64位的hash值，大于等于DCT均值的设为”1”，小于DCT均值的设为“0”。

![2-1-6](.\assets\2-1-6.png)
图2-1-6：phash各步骤效果图

  具体而言，对于每个图像进行phash计算后都将得到一个64位的二进制hash值，如果两个hash值不同的位数小于8，我们就认为这两个图像是相似的，反之则是不相似。对于视频每一帧，我们比较相邻的10帧，如果都相似则剔除重复的帧，否则进行保留。对保留下来的帧，进行文字检测，得到若干个文字框图像。对这些文字框图像进行类似的去重操作，与相邻100个文字框图像比较去重，在这个过程中注意保存下时间戳。最后对文字框图像进行文字识别，得到图像文本。

#### 音频文本与图像文本融合

通过音频文本与图像文本对应的时间戳进行归并对齐，我们通过两个指针分别遍历音频文本和图像文本，若某一个指针对应的时间戳小于另一个指针的时间戳，那么就处理该指针对应的数据。将每个待添加的语句，依次已添加的语句，通过**编辑距离**判定相似度。去掉相似度过高的图像文本，实现文本去重。将得到的结果拼接得到视频文本。

**编辑距离**计算：编辑距离，即Levenshtein Distance，指两个字串之间，由一个转换成另一个所需的最少编辑操作次数。我们使用动态规划算法。
